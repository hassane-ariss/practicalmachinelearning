

Practical Machine Learning Project
==================================================================
Project report for Practic Machine Learning course on coursera

author: "Hassane ARISS"
date: "30 octobre 2016"
GitHub repo: "https://github.com/hassane-ariss/practicalmachinelearning"

##Introduction
The goal of the project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. 
Any of the other variables may be used to predict with. This report is describing how the model was build, how cross validation was ued , and what are the choices made.


##Data Sources
The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project comes from this original source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

Please Note that I the code I use loads the data directly from the URL provided, so that you are not required to download the file to your environment. Please customize the code to your specific needs.



##Reproduceablity
In order to reproduce the same results, you need a certain set of packages, as well as setting a pseudo random seed equal to the one I used. *Note:To install, for instance, the caret package in R, run this command: install.packages(“caret”)

The following Libraries were used for this project, which you should install - if not done yet - and load on your working environment.


```{r}
library(caret)
```
```{r}
set.seed(12345)
```

###Loading the Data 

```{r, echo=FALSE}
setwd("C:/Users/hariss.CASDOMATSIEGE/Dropbox/Coursera/DatascienceSpecialization/PracticalMachineLearning/project/PracticalMachineLearning")
```

```
PML_Training = read.csv('Data/pml-training.csv', header = TRUE , na.strings=c("NA","#DIV/0!",""))
PML_Testing = read.csv('Data/pml-testing.csv', header = TRUE , na.strings=c("NA","#DIV/0!",""))

```
### Partitionning the training set ################################

```
inTrain <- createDataPartition(y=PML_Training$classe, p=0.6, list=FALSE)
myTraining <- PML_Training[inTrain, ]; myTesting <- PML_Training[-inTrain, ]

```

## Cleaning 
#### Cleaning Training dataset
```
############## Removing nearzerovariance predictors ###############

myDataNZV <- nearZeroVar(myTraining)
if(length(myDataNZV) > 0) myTraining <- myTraining[,-myDataNZV]
```

```
########## Removing ID column to not being considered in training ##########

myTraining <- myTraining [c(-1)]

```

```
##########  Removing predictors (columns) with NA ##################

cols.without.na <- colSums(is.na(myTraining)) == 0
myTraining <- myTraining[, cols.without.na]

``` 


#### Cleaning Test dataset 

```{}
mytraining_cols <- colnames(myTraining)
myTesting <- myTesting[mytraining_cols]
``` 


##  Creating models 

####Using LDA algorithm

```
fitlda <- train(classe ~ . , data= myTraining , method = "lda")
```

```
predlda <- predict(fitlda, myTesting)
```

Evaluation the performance against the test set

```{}
confusionMatrix(predlda,myTesting$classe)

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2025  162    3    0    0
         B  183 1112  120    1    0
         C   24  234 1212  135    3
         D    0   10   31 1076  135
         E    0    0    2   74 1304

Overall Statistics
                                          
               Accuracy : 0.8576          
                 95% CI : (0.8497, 0.8653)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.8201          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9073   0.7325   0.8860   0.8367   0.9043
Specificity            0.9706   0.9520   0.9389   0.9732   0.9881
Pos Pred Value         0.9247   0.7853   0.7537   0.8594   0.9449
Neg Pred Value         0.9634   0.9369   0.9750   0.9682   0.9787
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2581   0.1417   0.1545   0.1371   0.1662
Detection Prevalence   0.2791   0.1805   0.2049   0.1596   0.1759
Balanced Accuracy      0.9389   0.8423   0.9124   0.9049   0.9462

```

The accuracy of this model is 85.76% , an error of more than 4%


####Using GBM algorithm

```
fitgbm <- train(classe ~ . , data= myTraining , method = "gbm")
```

```
predgbm <- predict(fitgbm, myTesting)
```

Evaluation the performance against the test set

```
confusionMatrix(predgbm, myTesting$classe)

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2231    4    0    0    0
         B    1 1512    1    0    0
         C    0    2 1361    7    0
         D    0    0    6 1270    0
         E    0    0    0    9 1442

Overall Statistics
                                          
               Accuracy : 0.9962          
                 95% CI : (0.9945, 0.9974)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9952          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9996   0.9960   0.9949   0.9876   1.0000
Specificity            0.9993   0.9997   0.9986   0.9991   0.9986
Pos Pred Value         0.9982   0.9987   0.9934   0.9953   0.9938
Neg Pred Value         0.9998   0.9991   0.9989   0.9976   1.0000
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2843   0.1927   0.1735   0.1619   0.1838
Detection Prevalence   0.2849   0.1930   0.1746   0.1626   0.1849
Balanced Accuracy      0.9994   0.9979   0.9967   0.9933   0.9993
```

The accuracy of this model is 99.62% , an error of less than 1%, wich is pretty good.


####Using RF algorithm

```
fitRf <- train(classe ~ . , data= myTraining , method = "rf")
```

```
predRf <- predict(fitRf, myTesting)
```

Evaluation the performance against the test set
```
confusionMatrix(predRf,myTesting$classe)

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2232    1    0    0    0
         B    0 1517    2    0    0
         C    0    0 1366    3    0
         D    0    0    0 1283    0
         E    0    0    0    0 1442

Overall Statistics
                                          
               Accuracy : 0.9992          
                 95% CI : (0.9983, 0.9997)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.999           
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            1.0000   0.9993   0.9985   0.9977   1.0000
Specificity            0.9998   0.9997   0.9995   1.0000   1.0000
Pos Pred Value         0.9996   0.9987   0.9978   1.0000   1.0000
Neg Pred Value         1.0000   0.9998   0.9997   0.9995   1.0000
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2845   0.1933   0.1741   0.1635   0.1838
Detection Prevalence   0.2846   0.1936   0.1745   0.1635   0.1838
Balanced Accuracy      0.9999   0.9995   0.9990   0.9988   1.0000

```

The accuracy of this model is 99.92% , an error of less than 0.1%, wich is the best of the tree algorithms.

##Predicting the Project cases

As the Random Forest is the more accurate algorithm to predict based on the training set, it will be used to predict the test cases of the project

Test data will be arranged to fit the used model (removing the unecessary columns) 

```
mytesting_cols <- mytraining_cols[-58]
real_test <- PML_Testing[mytesting_cols]
```

predict the values based on the RF model 
```
predict(fitRf,real_test)
 [1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
```



